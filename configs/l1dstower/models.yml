# ML model definitions
#
# m.mieskolainen@imperial.ac.uk, 2024

# ** Model save and load parameters **
EVAL_PARAM: &EVAL_PARAM
  
  tensorboard: true

  # Training and validation
  savemode: 1                  # Save and evaluate model file every n-th epoch (integer)
                               # (BDT will save always only the last one because it contains all epochs)
  eval_batch_size: 1024
  
  # Post-training evaluation
  readmode: -1                 # specific epoch (int), -1 finds the best loss model, -2 takes the last epoch model
  readmode_metric: 'loss'      # e.g. 'loss', 'AUC' ... [currently only 'loss']
  readmode_operator: 'argmin'  # 


# -----------------------------------------------
# ** Main Domain Filter **
MAIN_DOMAIN_FILTER: &MAIN_DOMAIN_FILTER
  
  filter[0]:
    operator: null   # null (unary), cartesian_and', 'cartesian_or'
    sets: [0]        # input set ids (one or several)
  
  # ... more filters here
  
  # --------------------
  
  set[0]:
    expand: 'set'    # 'set', 'vetoset' or 'powerset'
    cutset:
      # Latex description is for boolean [0,1] per cut
      - {cut: 'Jet_pt_0 > 0', latex: ['', 'Jet_pt_0 > 0']}

# -----------------------------------------------


# -----------------------------------------------
# ** Mutual Information Control Domain Filter **
MI_DOMAIN_FILTER: &MI_DOMAIN_FILTER
  
  filter[0]:
    operator: null  # null (unary), 'cartesian_and', 'cartesian_or'
    sets: [0]       # input set ids (one or several)
  
  # ... more filters here
  
  # --------------------
  
  set[0]:
    expand: 'set'  # 'set', 'vetoset' or 'powerset'
    cutset:
      # Latex description is for boolean [0,1] per cut
      - {cut: 'Jet_pt_0 > 0', latex: ['', 'Jet_pt_0 > 0']}
  
  #set[0]:
  #  expand: 'set'   # 'set', 'vetoset' or 'powerset'
  #  cutset:
  #    [{cut: '0 < muonSV_mass_0 && muonSV_dxy_0 < 1.0  && muonSV_pAngle_0 < 0.2', latex: ['', '$\Delta_{xy} < 1$ & $\delta < 0.2$']},
  #     {cut: '0 < muonSV_mass_0 && muonSV_dxy_0 < 1.0  && muonSV_pAngle_0 > 0.2', latex: ['', '$\Delta_{xy} < 1$ & $\delta > 0.2$']},
  #     {cut: '0 < muonSV_mass_0 && 1.0  < muonSV_dxy_0 && muonSV_dxy_0 < 10 && muonSV_pAngle_0 < 0.2', latex: ['', '$1 < \Delta_{xy} < 10$ & $\delta < 0.2$']},
  #     {cut: '0 < muonSV_mass_0 && 1.0  < muonSV_dxy_0 && muonSV_dxy_0 < 10 && muonSV_pAngle_0 > 0.2', latex: ['', '$1 < \Delta_{xy} < 10$ & $\delta > 0.2$']},
  #     {cut: '0 < muonSV_mass_0 && muonSV_dxy_0 > 10.0 && muonSV_pAngle_0 < 0.2', latex: ['', '$\Delta_{xy} > 10$ & $\delta < 0.2$']},
  #     {cut: '0 < muonSV_mass_0 && muonSV_dxy_0 > 10.0 && muonSV_pAngle_0 > 0.2', latex: ['', '$\Delta_{xy} > 10$ & $\delta > 0.2$']}
  #    ]
# -----------------------------------------------


# -----------------------------------------------
# ** Mutual Information regularization **
MI_REG_PARAM: &MI_REG_PARAM
  
  #classes: [0,-2]          #  Which classes to use in the regularization
  #classes: [0]
  classes: [0, 1]
  
  losstype: 'DCORR'        # 'MINE', 'MINE_EMA', 'DENSITY', 'DCORR', 'PEARSON' (use PEARSON only for debug)
  min_count: 32            #  Minimum number of events (per category)
  max_N: 5000              #  Maximum number of events per function call (use to limit sample size for DCORR)
  min_score: 0.0           #  Consider only events with MVA output [0,1] larger than this value (null for None)
  poisson_weight: False    #  Per category Poisson sqrt(N) weighted loss
  
  # ------------------
  # Neural MI param

  eval_batch_size: 8192    #  Evaluation batch size (pure memory <-> time tradeoff)
  alpha:  0.01             #  Exponential Moving Average (EMA) coupling parameter
  ma_eT:  [null, null]     #  EMA tracking values (set for each class!)
  
  y_dim: [1]               #  dimensionality ([1] for NN scalar output Z vs target X)
  
  epochs: 5
  batch_size: 256          #  if the estimate (network) turns into NaN, try tuning batch size, lr, weight decay ...
  lr: 1.0e-4
  weight_decay: 1.0e-2
  clip_norm: 1.0
  
  mlp_dim: [128, 128]
  batch_norm: False
  dropout: 0.01
  noise_std: 0.025
  activation: 'relu'
  
  # -------------------------
  
  # Comment out for no categories (N.B. dependence structure over inclusive sample != categorical)
  set_filter: *MI_DOMAIN_FILTER
# -----------------------------------------------

# ========================================================================
## Classifier setup
# ========================================================================

cutset0:
  train:   'cutset'
  predict: 'cutset'
  
  label:   'cutset'
  raytune:  null

  cutstring: >-
    dijet_deta < 1.1
  
cut0:
  train:  'cut'
  predict: 'cut'
  label:  'dijet_deta'
  variable: 'dijet_deta'
  sign: -1
  transform: null

# ------------------------------------------------------------------------------------------


# XGBoost
# https://xgboost.readthedocs.io/en/latest/parameter.html
xgb01: &XGB01
  train:   'xgb'
  predict: 'xgb'
  label:   'XGB'
  raytune:  xgb_trial_0
  
  #exclude_MVA_vars: null
  exclude_MVA_vars: ['dijet.*']
  #exclude_MVA_vars: ['dijet.*', 'L1Jet_pt.*']
  #include_MVA_vars: ['.*']
  
  # general parameters
  model_param:
    num_boost_round: 100       # number of epochs (equal to the number of trees!)
    booster: 'gbtree'          # 'gbtree' (default), 'dart' (dropout boosting)
    tree_method: 'hist'
    device:      'auto'        # 'auto', 'cpu', 'cuda'
    
    learning_rate: 0.1
    gamma: 0.0
    max_depth: 3
    min_child_weight: 1.0
    max_delta_step: 0
    subsample: 1

    colsample_bytree:  1.0
    colsample_bylevel: 1.0
    colsample_bynode:  1.0
    
    reg_lambda: 1.0               # L2 regularization
    reg_alpha: 0.00               # L1 regularization
    
    # learning task parameters
    objective: 'binary:logistic'               # Note that 'multi:softprob' does not work with distillation
    eval_metric: ['logloss']                   
    #objective: 'custom:binary_cross_entropy'  # Use 'custom:binary_cross_entropy:hessian' for 2nd order descent
    #eval_metric: ['custom']                   # for custom losses, otherwise 'logloss', 'mlogloss' ...
  
  plot_trees: false

  <<: *EVAL_PARAM

# XGBoost with only L1Jet variables
xgb01_onlyl1jet: &XGB01_ONLYL1JET
  train:   'xgb'
  predict: 'xgb'
  label:   'XGB_ONLYL1JET'
  raytune:  xgb_trial_0
  
  #exclude_MVA_vars: null
  exclude_MVA_vars: ['dijet.*', 'nL1EmulCaloTower', 'L1EmulCaloTower.*']
  #exclude_MVA_vars: ['dijet.*', 'nL1EmulCaloTower', 'L1EmulCaloTower.*', 'L1Jet_pt.*']
  #include_MVA_vars: ['.*']
  
  # general parameters
  model_param:
    num_boost_round: 100       # number of epochs (equal to the number of trees!)
    booster: 'gbtree'          # 'gbtree' (default), 'dart' (dropout boosting)
    tree_method: 'hist'
    device:      'auto'        # 'auto', 'cpu', 'cuda'
    
    learning_rate: 0.1
    gamma: 0.0
    max_depth: 3
    min_child_weight: 1.0
    max_delta_step: 0
    subsample: 1

    colsample_bytree:  1.0
    colsample_bylevel: 1.0
    colsample_bynode:  1.0
    
    reg_lambda: 1.0               # L2 regularization
    reg_alpha: 0.00               # L1 regularization
    
    # learning task parameters
    objective: 'binary:logistic'               # Note that 'multi:softprob' does not work with distillation
    eval_metric: ['logloss']                   
    #objective: 'custom:binary_cross_entropy'  # Use 'custom:binary_cross_entropy:hessian' for 2nd order descent
    #eval_metric: ['custom']                   # for custom losses, otherwise 'logloss', 'mlogloss' ...
  
  plot_trees: false

  <<: *EVAL_PARAM

# ICEBOOST (XGB with torch driven custom loss)
# https://xgboost.readthedocs.io/en/latest/parameter.html
iceboost01: &ICEBOOST01
  train:   'xgb'
  predict: 'xgb_logistic'
  label:   'ICEBOOST'
  raytune:  xgb_trial_0

  #exclude_MVA_vars: null
  exclude_MVA_vars: ['dijet_m']
  #include_MVA_vars: ['.*']
  
  # general parameters
  model_param:
    num_boost_round: 200       # number of epochs (equal to the number of trees!)
    booster: 'gbtree'         # 'gbtree' (default), 'dart' (dropout boosting)
    tree_method: 'hist'
    device:      'auto'       # 'auto', 'cpu', 'cuda'
    
    learning_rate: 3e-2
    gamma: 0.5
    max_depth: 5
    min_child_weight: 1.0
    max_delta_step: 1
    subsample: 1

    colsample_bytree:  0.86
    colsample_bylevel: 0.6
    colsample_bynode:  0.8
    
    reg_lambda: 2.0               # L2 regularization
    reg_alpha: 0.05               # L1 regularization
    
    # learning task parameters
    #objective: 'binary:logistic'             # Note that 'multi:softprob' does not work with distillation
    #eval_metric: ['logloss']                 
    objective: 'custom:binary_cross_entropy'  # Use 'custom:binary_cross_entropy:hessian' for 2nd order descent
    eval_metric: ['custom']                   # for custom losses, otherwise 'logloss', 'mlogloss' ...
  
  # BCE loss domains
  BCE_param:
    main:
      classes: [0, 1]
      beta: 1.0
      #set_filter: *MAIN_DOMAIN_FILTER # Comment out for 'inclusive'
  
  plot_trees: false

  <<: *EVAL_PARAM

# ICEBOOST with MI-regularization
#
iceboost01-MI:

  <<: *ICEBOOST01

  label:   'ICEBOOST-MI'

  exclude_MVA_vars: ['dijet_m']
  #exclude_MVA_vars: ['dijet_m', 'dijet_deta', "Jet_pt.*"]
  #include_MVA_vars: ['.*']
  
  MI_param:
    beta: [0.1, 0.1]         # Positive for minimizing (per class in use)
    <<: *MI_REG_PARAM